{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRE CLEAR Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3PNzCiRxxVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV_DjSd_Ezxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install allennlp\n",
        "# !pip install git+https://github.com/erikavaris/tokenizer.git \n",
        "# !wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5 -P ./data/external\n",
        "# !wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json -P ./data/external"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojeuM1oJW5MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\"\"\"\n",
        "Functions for loading the RumorEval dataset.\n",
        "\n",
        "Requires that the source files are manually placed in the `EXTERNAL_DATA_DIR`\n",
        "folder. See the README for details.\n",
        "\n",
        "Data is read directly from the .zip-files without extracting them, because this\n",
        "was deemed more elegant.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "from enum import Enum\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from sys import exit\n",
        "from time import time\n",
        "from typing import Dict, List, Optional\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from tokenizer.tokenizer import RedditTokenizer, TweetTokenizer\n",
        "\n",
        "DATA_DIR = Path('/content/drive/My Drive/IRE dataset')\n",
        "\n",
        "EXTERNAL_DATA_DIR = DATA_DIR\n",
        "ELMO_WEIGHTS_FILE = (EXTERNAL_DATA_DIR\n",
        "                     / 'elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5')\n",
        "ELMO_OPTIONS_FILE = (EXTERNAL_DATA_DIR\n",
        "                     / 'elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json')\n",
        "\n",
        "TRAINING_DATA_ARCHIVE_FILE = (EXTERNAL_DATA_DIR\n",
        "                              / 'rumoureval-2019-training-data.zip')\n",
        "TEST_DATA_ARCHIVE_FILE = (EXTERNAL_DATA_DIR\n",
        "                          / 'rumoureval-2019-test-data.zip')\n",
        "EVALUATION_DATA_FILE = EXTERNAL_DATA_DIR / 'final-eval-key.json'\n",
        "EVALUATION_SCRIPT_FILE = EXTERNAL_DATA_DIR / 'home_scorer_macro.py'\n",
        "\n",
        "\n",
        "def check_for_required_external_data_files() -> None:\n",
        "    \"\"\"Checks whether all required external data files are present.\n",
        "\n",
        "    If not, will print a message to stderr and exit.\n",
        "    \"\"\"\n",
        "    for required_file in [ELMO_WEIGHTS_FILE, ELMO_OPTIONS_FILE,\n",
        "                          TRAINING_DATA_ARCHIVE_FILE, TEST_DATA_ARCHIVE_FILE, EVALUATION_SCRIPT_FILE] :\n",
        "                          # EVALUATION_SCRIPT_FILE]:\n",
        "        if not required_file.exists():\n",
        "            exit('Required file \"{}\" is not present. See the README on how to '\n",
        "                 'obtain it.'.format(required_file))\n",
        "\n",
        "\n",
        "TOKENIZER_ARGS = {\n",
        "    'preserve_case': False,\n",
        "    'preserve_handles': False,\n",
        "    'preserve_hashes': False,\n",
        "    'preserve_len': False,\n",
        "    'preserve_url': False,\n",
        "}\n",
        "TWEET_TOKENIZER = TweetTokenizer(**TOKENIZER_ARGS)\n",
        "REDDIT_TOKENIZER = RedditTokenizer(**TOKENIZER_ARGS)\n",
        "\n",
        "\n",
        "class Post:\n",
        "    \"\"\"Data class for both Twitter and Reddit posts.\n",
        "\n",
        "    Args:\n",
        "        id: ID of the post.\n",
        "        text: Tokenized text of the the post for Twitter, title/body for Reddit.\n",
        "        depth: Depth in the thread. Source posts always have `depth=0`, replies\n",
        "            to source posts have `depth=1`, replies to replies have `depth=2`,\n",
        "            and so forth.\n",
        "        platform: Whether the post is from Twitter or from Reddit.\n",
        "        has_media: `True` if the posts links to any media, `False` otherwise.\n",
        "        source_id: The ID of the source post of the thread. If the current post\n",
        "            is itself a source post, this is equal to `self.id`.\n",
        "        topic: The rumor topic the posts belongs to for Twitter. `None` for\n",
        "            Reddit posts, since the dataset has no topic labels for them.\n",
        "        user_verified: Whether the user is a verified Twitter user. `None` for\n",
        "            Reddit posts, since the dataset does not contain any info on this.\n",
        "        followers_count: For Twitter posts, the number of followers the author\n",
        "            of the post has. `None` for Reddit posts, since the concept doesn't\n",
        "            exist for Reddit.\n",
        "        friends_count: For Twitter posts, the number of accounts the author of\n",
        "            the post is following. `None` for Reddit posts, since the concept\n",
        "            doesn't exist for Reddit.\n",
        "        upvote_ratio: The upvote ratio for Reddit posts. `None` for Twitter\n",
        "            posts, since the concept doesn't exist for Twitter.\n",
        "    \"\"\"\n",
        "\n",
        "    class Platform(Enum):\n",
        "        \"\"\"Enum to designate whether a posts is from Twitter or from Reddit.\"\"\"\n",
        "        twitter = 1\n",
        "        reddit = 2\n",
        "\n",
        "    def __init__(self,\n",
        "                 id: str,\n",
        "                 text: str,\n",
        "                 depth: int,\n",
        "                 platform: Platform,\n",
        "                 has_media: bool,\n",
        "                 source_id: Optional[str] = None,\n",
        "                 topic: Optional[str] = None,\n",
        "                 user_verified: Optional[bool] = None,\n",
        "                 followers_count: Optional[int] = None,\n",
        "                 friends_count: Optional[int] = None,\n",
        "                 upvote_ratio: Optional[float] = None):\n",
        "        self.id = id\n",
        "\n",
        "        if platform == self.Platform.twitter:\n",
        "            self.text: List[str] = TWEET_TOKENIZER.tokenize(text)\n",
        "        elif platform == self.Platform.reddit:\n",
        "            self.text: List[str] = REDDIT_TOKENIZER.tokenize(text)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "\n",
        "        self.depth = depth\n",
        "        self.platform = platform\n",
        "        self.has_media = has_media\n",
        "        self.source_id = source_id or self.id\n",
        "        self.topic = topic\n",
        "        self.user_verified = user_verified\n",
        "        self.followers_count = followers_count\n",
        "        self.friends_count = friends_count\n",
        "        self.upvote_ratio = upvote_ratio\n",
        "\n",
        "    @property\n",
        "    def has_source_depth(self) -> bool:\n",
        "        \"\"\"Whether the post is the source of a thread.\"\"\"\n",
        "        return self.depth == 0\n",
        "\n",
        "    @property\n",
        "    def has_reply_depth(self) -> bool:\n",
        "        \"\"\"Whether the post is a reply to the source of a thread.\"\"\"\n",
        "        return self.depth == 1\n",
        "\n",
        "    @property\n",
        "    def has_nested_depth(self) -> bool:\n",
        "        \"\"\"Whether the post is neither source nor reply to a thread's source.\"\"\"\n",
        "        return self.depth >= 2\n",
        "\n",
        "    @property\n",
        "    def url(self) -> str:\n",
        "        \"\"\"Url of the post (useful for debugging).\"\"\"\n",
        "        if self.platform == self.Platform.twitter:\n",
        "            return 'https://twitter.com/statuses/{}'.format(self.id)\n",
        "        elif self.platform == self.Platform.reddit:\n",
        "            if self.source_id == self.id:\n",
        "                return 'https://reddit.com//comments/{}'.format(self.id)\n",
        "            return 'https://reddit.com//comments/{}//{}'.format(self.source_id,\n",
        "                                                                self.id)\n",
        "        raise ValueError('Invalid post source value, must be either Twitter or '\n",
        "                         'Reddit.')\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return 'Post {}'.format(vars(self))\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_twitter_dict(cls,\n",
        "                               twitter_dict: Dict,\n",
        "                               post_depths: Dict[str, int],\n",
        "                               source_id: Optional[str] = None,\n",
        "                               topic: Optional[str] = None) -> 'Post':\n",
        "        \"\"\"Creates a `Post` instance from a JSON dict of a Twitter post.\n",
        "\n",
        "        Args:\n",
        "            twitter_dict: The JSON dict.\n",
        "            post_depths: A map that gives the depth of the post by it's ID.\n",
        "            source_id: The ID of the thread's source post. `None` if this post\n",
        "                is itself the source post.\n",
        "            topic: The rumor topic the posts is labelled to belong to.\n",
        "\n",
        "        Returns:\n",
        "            The created `Post` instance.\n",
        "        \"\"\"\n",
        "        id = twitter_dict['id_str']\n",
        "        return Post(id=id,\n",
        "                    text=twitter_dict['text'],\n",
        "                    depth=post_depths[id],\n",
        "                    platform=cls.Platform.twitter,\n",
        "                    has_media='media' in twitter_dict['entities'],\n",
        "                    source_id=source_id,\n",
        "                    topic=topic,\n",
        "                    user_verified=twitter_dict['user']['verified'],\n",
        "                    followers_count=twitter_dict['user']['followers_count'],\n",
        "                    friends_count=twitter_dict['user']['friends_count'])\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_reddit_dict(cls,\n",
        "                              reddit_dict: Dict,\n",
        "                              post_depths: Dict[str, int],\n",
        "                              source_id: Optional[str] = None) -> 'Post':\n",
        "        \"\"\"Creates a `Post` instance from a JSON dict of a Reddit post.\n",
        "\n",
        "        There are labels for some deleted Reddit posts (all classified as\n",
        "        \"comment\"). For these posts only the ID is available. The text is set\n",
        "        to be empty. See:\n",
        "        https://groups.google.com/forum/#!msg/rumoureval/-6XzTDhWirk/eSc31xFOFQAJ\n",
        "\n",
        "        Args:\n",
        "            reddit_dict: The JSON dict.\n",
        "            post_depths: A map that gives the depth of the post by it's ID.\n",
        "            source_id: The ID of the thread's source post. `None` if this post\n",
        "                is itself the source post.\n",
        "\n",
        "        Returns:\n",
        "            The created `Post` instance.\n",
        "        \"\"\"\n",
        "        data = reddit_dict['data']\n",
        "        if 'children' in data and isinstance(data['children'][0], dict):\n",
        "            data = data['children'][0]['data']\n",
        "\n",
        "        id = data['id']\n",
        "        return Post(id=id,\n",
        "                    text=data.get('title') or data.get('body') or '',\n",
        "                    depth=post_depths[id],\n",
        "                    platform=cls.Platform.reddit,\n",
        "                    has_media=('domain' in data\n",
        "                               and not data['domain'].startswith('self.')),\n",
        "                    source_id=source_id,\n",
        "                    upvote_ratio=data.get('upvote_ratio'))\n",
        "\n",
        "\n",
        "def load_posts() -> Dict[str, Post]:\n",
        "    \"\"\"Loads all Twitter and Reddit posts into a dictionary.\n",
        "\n",
        "    Since the dataset is very small, we just load the whole dataset into RAM.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping post IDs to their respective posts.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_archive_directory_structure(archive: ZipFile) -> Dict:\n",
        "        \"\"\"Parses a ZipFile's list of files into a hierarchical representation.\n",
        "\n",
        "        We need to do this because ZipFile just gives us a list of all files in\n",
        "        contains and doesn't provide any methods to check which files lie in a\n",
        "        specific subdirectory.\n",
        "\n",
        "        Args:\n",
        "            archive: The archive to parse.\n",
        "\n",
        "        Returns:\n",
        "            A nested dictionary. Keys of this dictionary are either file names\n",
        "            which point to their full path in the archive or directory names\n",
        "            which again point to a nested dictionary that contains their\n",
        "            contents.\n",
        "\n",
        "        Example:\n",
        "            If the archive would contain the following files::\n",
        "\n",
        "                ['foo.txt',\n",
        "                 'bar/bar.log',\n",
        "                 'bar/baz.out',\n",
        "                 'bar/boogy/text.out']\n",
        "\n",
        "            This would be transformed into the following hierarchical form::\n",
        "\n",
        "                {\n",
        "                    'foo.txt': 'foo.txt',\n",
        "                    'bar': {\n",
        "                        'bar.log': 'bar/bar.log',\n",
        "                        'baz.out': 'bar/baz.out',\n",
        "                        'boogy': {\n",
        "                            'text.out': 'bar/boogy/text.out'\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "        \"\"\"\n",
        "        result = {}\n",
        "        for file in archive.namelist():\n",
        "            # Skip directories in archive.\n",
        "            if file.endswith('/'):\n",
        "                continue\n",
        "\n",
        "            d = result\n",
        "            path = file.split('/')[1:]  # [1:] to skip top-level directory.\n",
        "            for p in path[:-1]:  # [:-1] to skip filename\n",
        "                if p not in d:\n",
        "                    d[p] = {}\n",
        "                d = d[p]\n",
        "            d[path[-1]] = file\n",
        "        return result\n",
        "\n",
        "    def calc_post_depths_from_thread_structure(thread_structure: Dict) \\\n",
        "            -> Dict[str, int]:\n",
        "        \"\"\"Calculates the nested depth of each post in a thread.\n",
        "\n",
        "        We determine post depth from the provided `structure.json` files in the\n",
        "        dataset because this is easier than following the chain of a post's\n",
        "        parents to the source post of a thread.\n",
        "\n",
        "        Args:\n",
        "            thread_structure: The parsed JSON dict from one of the dataset's\n",
        "                `structure.json` files.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary mapping post IDs to their nested depth. The source\n",
        "            post of a thread always has depth `0`, first level replies `1`, etc.\n",
        "\n",
        "        Example:\n",
        "            If the `thread_structure` would look like the following::\n",
        "\n",
        "                {\n",
        "                    'foo': {\n",
        "                        'bar': [],\n",
        "                        'baz': {\n",
        "                            'boogy': []\n",
        "                        },\n",
        "                        'qux': []\n",
        "                    }\n",
        "                }\n",
        "\n",
        "            The parsed post depths would be::\n",
        "\n",
        "                {\n",
        "                    'foo': 0,\n",
        "                    'bar': 1,\n",
        "                    'baz': 1,\n",
        "                    'boogy': 2,\n",
        "                    'qux': 1\n",
        "                }\n",
        "        \"\"\"\n",
        "        post_depths = {}\n",
        "\n",
        "        def walk(thread: Dict, depth: int) -> None:\n",
        "            for post_id, subthread in thread.items():\n",
        "                post_depths[post_id] = depth\n",
        "                if isinstance(subthread, Dict):\n",
        "                    walk(subthread, depth + 1)\n",
        "\n",
        "        walk(thread_structure, 0)\n",
        "        return post_depths\n",
        "\n",
        "    print('Loading posts...')\n",
        "    time_before = time()\n",
        "\n",
        "    training_data_archive = ZipFile(TRAINING_DATA_ARCHIVE_FILE)\n",
        "    training_data_contents = get_archive_directory_structure(\n",
        "        training_data_archive)\n",
        "    twitter_english = training_data_contents['twitter-english']\n",
        "    reddit_training_data = training_data_contents['reddit-training-data']\n",
        "    reddit_dev_data = training_data_contents['reddit-dev-data']\n",
        "\n",
        "    test_data_archive = ZipFile(TEST_DATA_ARCHIVE_FILE)\n",
        "    test_data_contents = get_archive_directory_structure(test_data_archive)\n",
        "    twitter_en_test_data = test_data_contents['twitter-en-test-data']\n",
        "    reddit_test_data = test_data_contents['reddit-test-data']\n",
        "\n",
        "    posts: Dict[str, Post] = {}\n",
        "\n",
        "    # -- Load Twitter posts ----------------------------------------------------\n",
        "    for archive, topics in [(training_data_archive, twitter_english.items()),\n",
        "                            (test_data_archive, twitter_en_test_data.items())]:\n",
        "        for topic, threads in topics:\n",
        "            for thread in threads.values():\n",
        "                post_depths = calc_post_depths_from_thread_structure(\n",
        "                    json.loads(archive.read(thread['structure.json'])))\n",
        "\n",
        "                source_post = Post.load_from_twitter_dict(\n",
        "                    json.loads(archive.read(\n",
        "                        next(iter(thread['source-tweet'].values())))),\n",
        "                    post_depths,\n",
        "                    topic=topic)\n",
        "                posts[source_post.id] = source_post\n",
        "\n",
        "                for reply in thread.get('replies', {}).values():\n",
        "                    reply_post = Post.load_from_twitter_dict(\n",
        "                        json.loads(archive.read(reply)),\n",
        "                        post_depths,\n",
        "                        source_id=source_post.id,\n",
        "                        topic=topic)\n",
        "                    posts[reply_post.id] = reply_post\n",
        "\n",
        "    # -- Load Reddit posts. ----------------------------------------------------\n",
        "    for archive, threads in [(training_data_archive,\n",
        "                              chain(reddit_training_data.values(),\n",
        "                                    reddit_dev_data.values())),\n",
        "                             (test_data_archive, reddit_test_data.values())]:\n",
        "        for thread in threads:\n",
        "            post_depths = calc_post_depths_from_thread_structure(\n",
        "                json.loads(archive.read(thread['structure.json'])))\n",
        "\n",
        "            source_post = Post.load_from_reddit_dict(\n",
        "                json.loads(archive.read(\n",
        "                    next(iter(thread['source-tweet'].values())))),\n",
        "                post_depths)\n",
        "            posts[source_post.id] = source_post\n",
        "\n",
        "            for reply in thread.get('replies', {}).values():\n",
        "                reply_post = Post.load_from_reddit_dict(\n",
        "                    json.loads(archive.read(reply)),\n",
        "                    post_depths,\n",
        "                    source_id=source_post.id)\n",
        "                posts[reply_post.id] = reply_post\n",
        "\n",
        "    print('  Number of posts: {:d} (Reddit={:d}, Twitter={:d})'.format(\n",
        "        len(posts),\n",
        "        sum(1 for p in posts.values() if p.platform == Post.Platform.reddit),\n",
        "        sum(1 for p in posts.values() if p.platform == Post.Platform.twitter)))\n",
        "    time_after = time()\n",
        "    print('  Took {:.2f}s.'.format(time_after - time_before))\n",
        "\n",
        "    return posts\n",
        "\n",
        "\n",
        "class SdqcInstance:\n",
        "    \"\"\"Data class for SDQC (RumorEval Task A) instances.\n",
        "\n",
        "    Args:\n",
        "        post_id: An ID referencing a Twitter or a Reddit post.\n",
        "        label: A label whether the stance expressed in the referenced post is\n",
        "            *support*, *deny*, *query*, or *comment* towards the rumor expressed\n",
        "            in the thread's source post.\n",
        "    \"\"\"\n",
        "\n",
        "    class Label(Enum):\n",
        "        \"\"\"Enum for SDQC labels `support`, `deny`, `query`, and `comment`.\"\"\"\n",
        "        support = 0\n",
        "        deny = 1\n",
        "        query = 2\n",
        "        comment = 3\n",
        "\n",
        "    def __init__(self, post_id: str, label: Optional[Label] = None):\n",
        "        self.post_id = post_id\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'SDQC ({}, {})'.format(self.post_id, self.label)\n",
        "\n",
        "\n",
        "def load_sdcq_instances() -> (List[SdqcInstance],\n",
        "                              List[SdqcInstance],\n",
        "                              Optional[List[SdqcInstance]]):\n",
        "    \"\"\"Load SDQC (RumorEval Task A) training, dev, and test datasets.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing lists of SDQC instances. The first element is the\n",
        "        training dataset, the second the dev, and the third the test, if it is\n",
        "        available, otherwise `None`.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_from_json_dict(json_dict: Dict[str, Dict[str, str]]) \\\n",
        "            -> List[SdqcInstance]:\n",
        "        return [SdqcInstance(post_id, SdqcInstance.Label[label])\n",
        "                for post_id, label in json_dict['subtaskaenglish'].items()]\n",
        "\n",
        "    training_data_archive = ZipFile(TRAINING_DATA_ARCHIVE_FILE)\n",
        "    train = load_from_json_dict(json.loads(training_data_archive.read(\n",
        "        'rumoureval-2019-training-data/train-key.json')))\n",
        "    dev = load_from_json_dict(json.loads(training_data_archive.read(\n",
        "        'rumoureval-2019-training-data/dev-key.json')))\n",
        "    test = None\n",
        "\n",
        "    if EVALUATION_DATA_FILE.exists():\n",
        "        with EVALUATION_DATA_FILE.open('rb') as fin:\n",
        "            test = load_from_json_dict(json.loads(fin.read()))\n",
        "\n",
        "    return train, dev, test\n",
        "\n",
        "\n",
        "class VerifInstance:\n",
        "    \"\"\"Data class for Verification (RumorEval Task B) instances.\n",
        "\n",
        "    Args:\n",
        "        post_id: An ID referencing a Twitter or a Reddit thread's source post.\n",
        "        label: A label whether the rumor expressed in the referenced post is\n",
        "            `true`, `false`, or `unverified`.\n",
        "    \"\"\"\n",
        "\n",
        "    class Label(Enum):\n",
        "        \"\"\" Enum for verification labels `true`, `false`, and `unverified`.\"\"\"\n",
        "        false = 0\n",
        "        true = 1\n",
        "        unverified = 2\n",
        "\n",
        "    def __init__(self, post_id: str, label: Optional[Label] = None):\n",
        "        self.post_id = post_id\n",
        "        self.label = label\n",
        "\n",
        "    def __str__(self):\n",
        "        print('Verif ({}, {})'.format(self.post_id, self.label))\n",
        "\n",
        "\n",
        "def load_verif_instances() -> (List[VerifInstance],\n",
        "                               List[VerifInstance],\n",
        "                               Optional[List[VerifInstance]]):\n",
        "    \"\"\"Load Verification (RumorEval Task B) training, dev, and test datasets.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing lists of Verification instances. The first element is\n",
        "        the training dataset, the second the dev, and the third the test, if it\n",
        "        is available, otherwise `None`.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_from_json_dict(json_dict: Dict[str, Dict[str, str]]) \\\n",
        "            -> List[VerifInstance]:\n",
        "        return [VerifInstance(post_id, VerifInstance.Label[label])\n",
        "                for post_id, label in json_dict['subtaskbenglish'].items()]\n",
        "\n",
        "    training_data_archive = ZipFile(TRAINING_DATA_ARCHIVE_FILE)\n",
        "    train = load_from_json_dict(json.loads(training_data_archive.read(\n",
        "        'rumoureval-2019-training-data/train-key.json')))\n",
        "    dev = load_from_json_dict(json.loads(training_data_archive.read(\n",
        "        'rumoureval-2019-training-data/dev-key.json')))\n",
        "    test = None\n",
        "\n",
        "    if EVALUATION_DATA_FILE.exists():\n",
        "        with EVALUATION_DATA_FILE.open('rb') as fin:\n",
        "            test = load_from_json_dict(json.loads(fin.read()))\n",
        "\n",
        "    return train, dev, test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1RjtLogXnr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "from collections import OrderedDict, defaultdict\n",
        "from enum import Enum\n",
        "from itertools import chain\n",
        "from math import sqrt\n",
        "from pathlib import Path\n",
        "from pprint import pprint\n",
        "from random import shuffle\n",
        "from sys import maxsize\n",
        "from time import time\n",
        "from typing import Callable, Dict, Iterable, List, Optional, Set, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# from src.dataset import ELMO_OPTIONS_FILE, ELMO_WEIGHTS_FILE, Post, \\\n",
        "#     SdqcInstance, VerifInstance\n",
        "\n",
        "\n",
        "class ScalingMode(Enum):\n",
        "    none = 0\n",
        "    min_max = 1\n",
        "    standard = 2\n",
        "\n",
        "\n",
        "class DatasetHelper(Dataset):\n",
        "    def __init__(self, post_embeddings: Dict[str, torch.tensor]):\n",
        "        super().__init__()\n",
        "        self._dataset = []\n",
        "        self._post_embeddings = post_embeddings\n",
        "\n",
        "    @classmethod\n",
        "    def calc_shared_features(cls, post: Post, post_embeddings: Dict[str, torch.Tensor]) \\\n",
        "            -> (np.ndarray, np.ndarray, np.ndarray):\n",
        "        post_platform = [post.platform == Post.Platform.twitter,\n",
        "                         post.platform == Post.Platform.reddit]\n",
        "\n",
        "        post_author = [0, 0, 0, 0, 0]\n",
        "        if post.platform == Post.Platform.twitter:\n",
        "            post_author = [post.user_verified,\n",
        "                           not post.user_verified,\n",
        "                           post.followers_count,\n",
        "                           post.friends_count,\n",
        "                           post.followers_count / (post.friends_count + 1e-8)]\n",
        "\n",
        "        post_similarity_to_source = np.array(1)\n",
        "        if not post.has_source_depth:\n",
        "            post_emb_mean = post_embeddings[post.id].mean(dim=1)\n",
        "            source_emb_mean = post_embeddings[post.source_id].mean(dim=1)\n",
        "            post_similarity_to_source = F.cosine_similarity(\n",
        "                post_emb_mean, source_emb_mean, dim=0).cpu().numpy()\n",
        "\n",
        "        return post_platform, post_author, post_similarity_to_source\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
        "        return self._dataset[index]\n",
        "\n",
        "    def calc_stats_for_aux_feature(self,\n",
        "                                   index: int,\n",
        "                                   filter_func: Optional[\n",
        "                                       Callable[[str], bool]] = None) \\\n",
        "            -> (float, float, float, float):\n",
        "        if not filter_func:\n",
        "            def filter_func(_post_id: str) -> bool:\n",
        "                return True\n",
        "\n",
        "        feature_values = np.array([post['features'][index].item()\n",
        "                                   for post in self._dataset\n",
        "                                   if filter_func(post['post_id'])])\n",
        "        return (feature_values.min(),\n",
        "                feature_values.max(),\n",
        "                feature_values.mean(),\n",
        "                feature_values.std())\n",
        "\n",
        "    def min_max_scale_aux_feature(self,\n",
        "                                  index: int,\n",
        "                                  min: float,\n",
        "                                  max: float,\n",
        "                                  filter_func: Optional[\n",
        "                                      Callable[[str], bool]] = None) \\\n",
        "            -> None:\n",
        "        if not filter_func:\n",
        "            def filter_func(_post_id: str) -> bool:\n",
        "                return True\n",
        "\n",
        "        for post in self._dataset:\n",
        "            if filter_func(post['post_id']):\n",
        "                value = post['features'][index]\n",
        "                post['features'][index] = (value - min) / (max - min)\n",
        "\n",
        "    def standard_scale_aux_feature(self,\n",
        "                                   index: int,\n",
        "                                   mean: float,\n",
        "                                   std: float,\n",
        "                                   filter_func: Optional[\n",
        "                                       Callable[[str], bool]] = None) \\\n",
        "            -> None:\n",
        "        if not filter_func:\n",
        "            def filter_func(_post_id: str) -> bool:\n",
        "                return True\n",
        "\n",
        "        for post in self._dataset:\n",
        "            if filter_func(post['post_id']):\n",
        "                value = post['features'][index]\n",
        "                post['features'][index] = (value - mean) / std\n",
        "\n",
        "\n",
        "def calculate_post_elmo_embeddings(posts: Dict[str, Post],\n",
        "                                   max_sentence_length: int,\n",
        "                                   batch_size: int,\n",
        "                                   scalar_mix_parameters: List[float],\n",
        "                                   device: torch.device,\n",
        "                                   elmo_options_file: Path = ELMO_OPTIONS_FILE,\n",
        "                                   elmo_weights_file: Path = ELMO_WEIGHTS_FILE\n",
        "                                   ) \\\n",
        "        -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Calculate ELMo embeddings of all posts in the dataset.\n",
        "\n",
        "    Calculating these embeddings one time before training the actual models\n",
        "    allows for extremely fast training later. The downsides are that we can't\n",
        "    propagate gradients through the embeddings, but fine-tuning these would\n",
        "    probably lead to be overfitting, since our dataset is very small.\n",
        "    Additionally, we also can't learn the scalar_mix_parameters, but since\n",
        "    training is so much faster, adjusting these by hand should be sufficient.\n",
        "\n",
        "    Since we are going to load the entire dataset into GPU memory later anyways,\n",
        "    we keep the embeddings in GPU memory here already.\n",
        "\n",
        "    Args:\n",
        "        posts: A dictionary mapping post IDs to their respective posts. Load\n",
        "            this with `src.dataset.load_posts()`.\n",
        "        max_sentence_length: Number of tokens after which sentences will be\n",
        "            truncated.\n",
        "        batch_size: Batch size for calculating the ELMo embeddings.\n",
        "        scalar_mix_parameters: Parameters for mixing the different ELMo layers.\n",
        "            See the paper for details on this.\n",
        "        device: Device to execute on.\n",
        "        elmo_options_file: file path to options for ELMo embeddings.\n",
        "        elmo_weights_file: file path to weights for ELMo embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping post IDs to their respective ELMo embedding in a\n",
        "        PyTorch tensor. Each tensor will have shape\n",
        "        `(num_elmo_dimensions, max_sentence_length)`.\n",
        "    \"\"\"\n",
        "\n",
        "    print('Calculating post embeddings...')\n",
        "    time_before = time()\n",
        "\n",
        "    elmo = Elmo(elmo_options_file,\n",
        "                elmo_weights_file,\n",
        "                num_output_representations=1,\n",
        "                dropout=0,\n",
        "                requires_grad=False,\n",
        "                do_layer_norm=False,\n",
        "                scalar_mix_parameters=scalar_mix_parameters).to(device)\n",
        "    elmo.eval()\n",
        "\n",
        "    post_embeddings = {}\n",
        "    batch_ids = []\n",
        "    # Add a dummy sentence with max_sentence_length to each batch to enforce\n",
        "    # that each batch of embeddings has the same shape. `batch_to_id()` and\n",
        "    # `elmo()` take care of zero padding shorter sentences for us.\n",
        "    batch_texts = [['' for _ in range(max_sentence_length)]]\n",
        "    for i, post in enumerate(posts.values()):\n",
        "        batch_ids.append(post.id)\n",
        "        batch_texts.append(post.text[:max_sentence_length])\n",
        "\n",
        "        if not i % batch_size or i == len(posts) - 1:\n",
        "            batch_character_ids = batch_to_ids(batch_texts).to(device)\n",
        "            batch_texts = [['' for _ in range(max_sentence_length)]]\n",
        "\n",
        "            # - [0] to select first output representation (there is only one\n",
        "            #   because of `num_output_representations=1` at `elmo` creation.\n",
        "            # - [1:] to ignore dummy sentence added at the start.\n",
        "            batch_embeddings = \\\n",
        "                elmo(batch_character_ids)['elmo_representations'][0][1:]\n",
        "            batch_embeddings = batch_embeddings.split(split_size=1, dim=0)\n",
        "            del batch_character_ids  # Free up memory sooner.\n",
        "\n",
        "            for post_id, post_embedding in zip(batch_ids, batch_embeddings):\n",
        "                post_embedding.squeeze_(dim=0)\n",
        "                post_embedding.transpose_(0, 1)\n",
        "                post_embeddings[post_id] = post_embedding\n",
        "            batch_ids = []\n",
        "\n",
        "    time_after = time()\n",
        "    print('  Took {:.2f}s.'.format(time_after - time_before))\n",
        "\n",
        "    return post_embeddings\n",
        "\n",
        "\n",
        "def generate_folds_for_k_fold_cross_validation(posts: Dict[str, Post],\n",
        "                                               num_folds: int) \\\n",
        "        -> List[Set[str]]:\n",
        "    posts_per_discriminator = defaultdict(set)\n",
        "    for post in posts.values():\n",
        "        if post.platform == Post.Platform.twitter:\n",
        "            discriminator = post.topic\n",
        "        elif post.platform == Post.Platform.reddit:\n",
        "            discriminator = post.source_id\n",
        "        else:\n",
        "            raise ValueError('Unimplemented enum variant.')\n",
        "        posts_per_discriminator[discriminator].add(post.id)\n",
        "    posts_per_discriminator = list(posts_per_discriminator.values())\n",
        "    shuffle(posts_per_discriminator)\n",
        "\n",
        "    folds = [set() for _ in range(num_folds)]\n",
        "    for post_ids in posts_per_discriminator:\n",
        "        # Find fold with fewest elements\n",
        "        index = None\n",
        "        num_elements = maxsize\n",
        "        for i, fold in enumerate(folds):\n",
        "            if num_elements > len(fold):\n",
        "                num_elements = len(fold)\n",
        "                index = i\n",
        "\n",
        "        # Add post to that fold\n",
        "        folds[index].update(post_ids)\n",
        "\n",
        "    return folds\n",
        "\n",
        "\n",
        "def arrange_folds_for_k_fold_cross_validation(folds: List[Set[str]],\n",
        "                                              index: int) \\\n",
        "        -> (Set[str], Set[str]):\n",
        "    train_post_ids = set(chain.from_iterable(\n",
        "        fold for i, fold in enumerate(folds) if i != index))\n",
        "    test_post_ids = folds[index]\n",
        "    return train_post_ids, test_post_ids\n",
        "\n",
        "\n",
        "def filter_instances(train_post_ids: Set[str],\n",
        "                     test_post_ids: Set[str],\n",
        "                     instances: Iterable[Union[SdqcInstance, VerifInstance]]) \\\n",
        "        -> (List[Union[SdqcInstance, VerifInstance]],\n",
        "            List[Union[SdqcInstance, VerifInstance]]):\n",
        "    train_instances = [i for i in instances if i.post_id in train_post_ids]\n",
        "    test_instances = [i for i in instances if i.post_id in test_post_ids]\n",
        "\n",
        "    shuffle(train_instances)\n",
        "    shuffle(test_instances)\n",
        "\n",
        "    return train_instances, test_instances\n",
        "\n",
        "\n",
        "def rmse_score(labels, predictions, confidences):\n",
        "    rmse = 0\n",
        "    for label, prediction, confidence in \\\n",
        "            zip(labels, predictions, confidences):\n",
        "        if label == prediction and \\\n",
        "                (label == VerifInstance.Label.true.value\n",
        "                 or label == VerifInstance.Label.false.value):\n",
        "            rmse += (1 - confidence) ** 2\n",
        "        elif label == VerifInstance.Label.unverified.value:\n",
        "            rmse += confidence ** 2\n",
        "        else:\n",
        "            rmse += 1\n",
        "    rmse = sqrt(rmse / len(labels))\n",
        "    return rmse\n",
        "\n",
        "\n",
        "def display_results(sdqc_accs: Iterable[float],\n",
        "                    sdqc_f1s: Iterable[float],\n",
        "                    sdqc_reports: Iterable[Dict[str, Dict[str, float]]],\n",
        "                    verif_accs: Iterable[float],\n",
        "                    verif_f1s: Iterable[float],\n",
        "                    verif_rmses: Iterable[float],\n",
        "                    verif_reports: Iterable[Dict[str, Dict[str, float]]]):\n",
        "    def display_report(reports: Iterable[Dict[str, Dict[str, float]]]):\n",
        "        report_lists = defaultdict(lambda: defaultdict(list))\n",
        "        for report in reports:\n",
        "            for outer_key, inner_report in report.items():\n",
        "                if outer_key == 'accuracy':\n",
        "                    report_lists[outer_key]['accuracy'].append(inner_report)\n",
        "                else:\n",
        "                    for inner_key, value in inner_report.items():\n",
        "                        report_lists[outer_key][inner_key].append(value)\n",
        "\n",
        "        report_stats = {}\n",
        "        for outer_key, inner_report in report_lists.items():\n",
        "            report_stats[outer_key] = {}\n",
        "            for inner_key, values in inner_report.items():\n",
        "                report_stats[outer_key][inner_key] = '{:.1%}±{:.1%}'.format(\n",
        "                    np.mean(values), np.std(values))\n",
        "\n",
        "        pprint(report_stats)\n",
        "\n",
        "    sdqc_acc = (np.mean(sdqc_accs), np.std(sdqc_accs))\n",
        "    sdqc_f1 = (np.mean(sdqc_f1s), np.std(sdqc_f1s))\n",
        "    print('Task A: SDQC')\n",
        "    print('  Accuracy: {:.1%}±{:.1%}'\n",
        "          '  F1-Score: {:.1%}±{:.1%}'\n",
        "          .format(sdqc_acc[0], sdqc_acc[1],\n",
        "                  sdqc_f1[0], sdqc_f1[1]))\n",
        "    \n",
        "    # print(sdqc_reports)\n",
        "    display_report(sdqc_reports)\n",
        "\n",
        "\n",
        "def write_answers_json(\n",
        "        path: Path,\n",
        "        sdqc_instances: List[SdqcInstance],\n",
        "        verif_instances: List[SdqcInstance],\n",
        "        sdqc_estimates: Dict[str, Tuple[SdqcInstance.Label,\n",
        "                                        Dict[SdqcInstance.Label, float]]],\n",
        "        verif_estimates: Dict[str, Tuple[VerifInstance.Label, float]]):\n",
        "    sdqc_answers = OrderedDict()\n",
        "    for instance in sdqc_instances:\n",
        "        answer = sdqc_estimates[instance.post_id]\n",
        "        sdqc_answers[instance.post_id] = answer[0].name\n",
        "\n",
        "    verif_answers = OrderedDict()\n",
        "    for instance in verif_instances:\n",
        "        answer = verif_estimates[instance.post_id]\n",
        "        verif_answers[instance.post_id] = (answer[0].name, answer[1])\n",
        "\n",
        "    answers = OrderedDict()\n",
        "    answers['subtaskaenglish'] = sdqc_answers\n",
        "    answers['subtaskbenglish'] = verif_answers\n",
        "\n",
        "    with path.open('w', encoding='UTF-8') as fout:\n",
        "        json.dump(answers, fout, indent=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8cpSHb5XHDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from src.dataset import Post, SdqcInstance\n",
        "# from src.util import DatasetHelper, ScalingMode\n",
        "\n",
        "EVAL_DEV_EVERY_N_EPOCH = 20\n",
        "\n",
        "\n",
        "class Sdqc:\n",
        "    class Hyperparameters:\n",
        "        def __init__(self,\n",
        "                     max_sentence_length: int,\n",
        "                     batch_size: int,\n",
        "                     num_epochs: int,\n",
        "                     learning_rate: float,\n",
        "                     weight_decay: float,\n",
        "                     class_weights: List[float],\n",
        "                     input_num_emb_dims: int,\n",
        "                     input_num_aux_dims: int,\n",
        "                     input_aux_scaling_features: List[int],\n",
        "                     input_aux_scaling_mode: ScalingMode,\n",
        "                     conv_num_layers: int,\n",
        "                     conv_kernel_sizes: List[int],\n",
        "                     conv_num_channels: int,\n",
        "                     dense_num_layers: int,\n",
        "                     dense_num_hidden: int,\n",
        "                     dense_dropout: float):\n",
        "            self.max_sentence_length = max_sentence_length\n",
        "            self.batch_size = batch_size\n",
        "            self.num_epochs = num_epochs\n",
        "            self.learning_rate = learning_rate\n",
        "            self.weight_decay = weight_decay\n",
        "            self.class_weights = class_weights\n",
        "            self.input_num_emb_dims = input_num_emb_dims\n",
        "            self.input_num_aux_dims = input_num_aux_dims\n",
        "            self.input_aux_scaling_features = input_aux_scaling_features\n",
        "            self.input_aux_scaling_mode = input_aux_scaling_mode\n",
        "            self.conv_num_layers = conv_num_layers\n",
        "            self.conv_kernel_sizes = conv_kernel_sizes\n",
        "            self.conv_num_channels = conv_num_channels\n",
        "            self.dense_num_layers = dense_num_layers\n",
        "            self.dense_num_hidden = dense_num_hidden\n",
        "            self.dense_dropout = dense_dropout\n",
        "\n",
        "    def __init__(self,\n",
        "                 posts: Dict[str, Post],\n",
        "                 post_embeddings: Dict[str, torch.Tensor],\n",
        "                 hparams: 'Sdqc.Hyperparameters',\n",
        "                 device: torch.device):\n",
        "        self._posts = posts\n",
        "        self._post_embeddings = post_embeddings\n",
        "        self._hparams = hparams\n",
        "        self._device = device\n",
        "\n",
        "    class Dataset(DatasetHelper):\n",
        "        def __init__(self,\n",
        "                     instances: Iterable[SdqcInstance],\n",
        "                     posts: Dict[str, Post],\n",
        "                     post_embeddings: Dict[str, torch.Tensor],\n",
        "                     hparams: 'Sdqc.Hyperparameters',\n",
        "                     device: torch.device):\n",
        "            super().__init__(post_embeddings)\n",
        "\n",
        "            for instance in instances:\n",
        "                post = posts[instance.post_id]\n",
        "                post_embedding = post_embeddings[post.id]\n",
        "\n",
        "                post_features = self.calc_features(post, post_embeddings)\n",
        "\n",
        "                self._dataset.append({\n",
        "                    'post_id': post.id,\n",
        "                    'emb': post_embedding,\n",
        "                    'features': torch.from_numpy(post_features).to(device),\n",
        "                    'label': (torch.tensor(instance.label.value, device=device)\n",
        "                              if instance.label else 0),\n",
        "                })\n",
        "\n",
        "        @classmethod\n",
        "        def calc_features(cls, post: Post, post_embeddings: Dict[str, torch.tensor]):\n",
        "            post_platform, post_author, post_similarity_to_source = \\\n",
        "                cls.calc_shared_features(post, post_embeddings)\n",
        "\n",
        "            post_type = [post.has_source_depth,\n",
        "                         post.has_reply_depth,\n",
        "                         post.has_nested_depth]\n",
        "\n",
        "            return (np.concatenate((post_platform,\n",
        "                                    post_author,\n",
        "                                    [post_similarity_to_source],\n",
        "                                    post_type))\n",
        "                    .astype(np.float32))\n",
        "\n",
        "    def build_datasets(self,\n",
        "                       train_instances: Iterable[SdqcInstance],\n",
        "                       dev_instances: Optional[Iterable[SdqcInstance]],\n",
        "                       test_instances: Optional[Iterable[SdqcInstance]]) \\\n",
        "            -> ('Sdqc.Dataset',\n",
        "                Optional['Sdqc.Dataset'],\n",
        "                Optional['Sdqc.Dataset']):\n",
        "        # print('Number of instances: train={:d}, dev={:d}, test={:d}'\n",
        "        #       .format(len(train_instances),\n",
        "        #               len(dev_instances or []),\n",
        "        #               len(test_instances or [])))\n",
        "\n",
        "        train_dataset = self.Dataset(\n",
        "            train_instances, self._posts, self._post_embeddings, self._hparams,\n",
        "            self._device)\n",
        "\n",
        "        dev_dataset = None\n",
        "        if dev_instances:\n",
        "            dev_dataset = self.Dataset(\n",
        "                dev_instances, self._posts, self._post_embeddings,\n",
        "                self._hparams, self._device)\n",
        "\n",
        "        test_dataset = None\n",
        "        if test_instances:\n",
        "            test_dataset = self.Dataset(\n",
        "                test_instances, self._posts, self._post_embeddings,\n",
        "                self._hparams, self._device)\n",
        "\n",
        "        def filter_func(post_id: str) -> bool:\n",
        "            return self._posts[post_id].platform == Post.Platform.twitter\n",
        "\n",
        "        for index in self._hparams.input_aux_scaling_features:\n",
        "            min, max, mean, std = \\\n",
        "                train_dataset.calc_stats_for_aux_feature(index, filter_func)\n",
        "\n",
        "            for dataset in (train_dataset, dev_dataset, test_dataset):\n",
        "                if not dataset:\n",
        "                    continue\n",
        "\n",
        "                if self._hparams.input_aux_scaling_mode == ScalingMode.none:\n",
        "                    pass\n",
        "                elif (self._hparams.input_aux_scaling_mode\n",
        "                      == ScalingMode.min_max):\n",
        "                    dataset.min_max_scale_aux_feature(\n",
        "                        index, min, max, filter_func)\n",
        "                elif (self._hparams.input_aux_scaling_mode\n",
        "                      == ScalingMode.standard):\n",
        "                    dataset.standard_scale_aux_feature(\n",
        "                        index, mean, std, filter_func)\n",
        "                else:\n",
        "                    raise ValueError('Unimplemented enum variant.')\n",
        "\n",
        "        return train_dataset, dev_dataset, test_dataset\n",
        "\n",
        "    class Model(nn.Module):\n",
        "        def __init__(self, hparams: 'Sdqc.Hyperparameters'):\n",
        "            super().__init__()\n",
        "            self._hparams = hparams\n",
        "\n",
        "            emb_num_output_dims = self._hparams.input_num_emb_dims\n",
        "\n",
        "            # -- convolutional layers ------------------------------------------\n",
        "            conv_num_input_dims = emb_num_output_dims\n",
        "            conv_num_output_dims = (len(self._hparams.conv_kernel_sizes)\n",
        "                                    * self._hparams.conv_num_channels)\n",
        "            self._conv_layers = nn.ModuleList()\n",
        "            for i in range(self._hparams.conv_num_layers):\n",
        "                layer = nn.ModuleDict()\n",
        "                for size in self._hparams.conv_kernel_sizes:\n",
        "                    conv = nn.Conv1d(\n",
        "                        in_channels=(conv_num_input_dims\n",
        "                                     if i == 0 else conv_num_output_dims),\n",
        "                        out_channels=self._hparams.conv_num_channels,\n",
        "                        kernel_size=size)\n",
        "                    batch_norm = nn.BatchNorm1d(\n",
        "                        num_features=self._hparams.conv_num_channels)\n",
        "                    layer['kernel_size{:d}'.format(size)] = nn.ModuleDict(\n",
        "                        {'conv': conv, 'batch_norm': batch_norm})\n",
        "                self._conv_layers.append(layer)\n",
        "\n",
        "            # -- dense layers --------------------------------------------------\n",
        "            if self._hparams.conv_num_layers:\n",
        "                dense_num_input_dims = \\\n",
        "                    conv_num_output_dims + self._hparams.input_num_aux_dims\n",
        "            else:\n",
        "                dense_num_input_dims = \\\n",
        "                    emb_num_output_dims + self._hparams.input_num_aux_dims\n",
        "            dense_num_output_dims = self._hparams.dense_num_hidden\n",
        "            self._dense_layers = nn.ModuleList()\n",
        "            for i in range(self._hparams.dense_num_layers):\n",
        "                self._dense_layers.append(nn.Linear(\n",
        "                    in_features=(dense_num_input_dims\n",
        "                                 if i == 0 else dense_num_output_dims),\n",
        "                    out_features=self._hparams.dense_num_hidden))\n",
        "\n",
        "            # -- linear layer --------------------------------------------------\n",
        "            if self._hparams.dense_num_layers:\n",
        "                linear_num_input_dims = dense_num_output_dims\n",
        "            elif self._hparams.conv_num_layers:\n",
        "                linear_num_input_dims = \\\n",
        "                    conv_num_output_dims + self._hparams.input_num_aux_dims\n",
        "            else:\n",
        "                linear_num_input_dims = \\\n",
        "                    emb_num_output_dims + self._hparams.input_num_aux_dims\n",
        "            self._linear = nn.Linear(\n",
        "                in_features=linear_num_input_dims,\n",
        "                out_features=len(SdqcInstance.Label))\n",
        "\n",
        "            # num_total_params = 0\n",
        "            # for i, (n, w) in enumerate(self.named_parameters()):\n",
        "            #     if w.requires_grad:\n",
        "            #         print(i, n, w.shape, w.numel())\n",
        "            #         num_total_params += w.numel()\n",
        "            # print('Num Total Parameters: {}'.format(num_total_params))\n",
        "\n",
        "        def forward(self, emb, aux):\n",
        "            x = emb\n",
        "\n",
        "            for layer in self._conv_layers:\n",
        "                h = []\n",
        "                for size in self._hparams.conv_kernel_sizes:\n",
        "                    conv_batch_norm = layer['kernel_size{:d}'.format(size)]\n",
        "                    conv = conv_batch_norm['conv']\n",
        "                    batch_norm = conv_batch_norm['batch_norm']\n",
        "\n",
        "                    h.append(batch_norm(F.relu(conv(\n",
        "                        F.pad(x, [(size - 1) // 2, size // 2])))))\n",
        "                x = torch.cat(h, dim=1)\n",
        "\n",
        "            x = F.avg_pool1d(x, kernel_size=self._hparams.max_sentence_length)\n",
        "            x.squeeze_(dim=2)\n",
        "\n",
        "            if self._hparams.input_num_aux_dims:\n",
        "                x = torch.cat((x, aux), dim=1)\n",
        "\n",
        "            for dense in self._dense_layers:\n",
        "                x = F.dropout(F.relu(dense(x)),\n",
        "                              p=self._hparams.dense_dropout,\n",
        "                              training=self.training)\n",
        "\n",
        "            logits = self._linear(x)\n",
        "\n",
        "            return logits\n",
        "\n",
        "    def train(self,\n",
        "              train_dataset: 'Sdqc.Dataset',\n",
        "              dev_dataset: Optional['Sdqc.Dataset'] = None,\n",
        "              print_progress: bool = False) -> 'Sdqc.Model':\n",
        "        model = self.Model(self._hparams).to(self._device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(\n",
        "            weight=torch.tensor(self._hparams.class_weights,\n",
        "                                dtype=torch.float32,\n",
        "                                device=self._device))\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=self._hparams.learning_rate,\n",
        "                               weight_decay=self._hparams.weight_decay)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=self._hparams.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch_no in range(1, self._hparams.num_epochs + 1):\n",
        "            losses, labels, predictions = [], [], []\n",
        "\n",
        "            progress_bar = None\n",
        "            if print_progress:\n",
        "                progress_bar = tqdm(total=(len(train_loader)),\n",
        "                                    unit='batch',\n",
        "                                    desc='Epoch: {:{}d}/{:d}'.format(\n",
        "                                        epoch_no,\n",
        "                                        len(str(self._hparams.num_epochs)),\n",
        "                                        self._hparams.num_epochs))\n",
        "\n",
        "            for batch_no, batch in enumerate(train_loader):\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                model.train()\n",
        "                batch_logits = model(batch['emb'], batch['features'])\n",
        "                with torch.no_grad():\n",
        "                    batch_prediction = torch.argmax(batch_logits, dim=1)\n",
        "\n",
        "                loss = criterion(batch_logits, batch['label'])\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                labels.append(batch['label'].data.cpu().numpy())\n",
        "                predictions.append(batch_prediction.data.cpu().numpy())\n",
        "\n",
        "                if progress_bar:\n",
        "                    progress_bar.set_postfix({\n",
        "                        'loss': '{:.2e}'.format(loss.item()),\n",
        "                    })\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "            if progress_bar:\n",
        "                progress_bar.close()\n",
        "\n",
        "                labels = np.concatenate(labels)\n",
        "                predictions = np.concatenate(predictions)\n",
        "\n",
        "                epoch_loss = np.mean(losses)\n",
        "                epoch_acc = accuracy_score(labels, predictions)\n",
        "                epoch_f1 = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "                print('  Loss={:.2e}  Accuracy={:.2%}  F1-score={:.2%}'\n",
        "                      .format(epoch_loss, epoch_acc, epoch_f1))\n",
        "\n",
        "            if print_progress and dev_dataset and \\\n",
        "                    (epoch_no == self._hparams.num_epochs\n",
        "                     or not epoch_no % EVAL_DEV_EVERY_N_EPOCH):\n",
        "                dev_acc, dev_f1, _ = self.eval(model, dev_dataset)\n",
        "                print('  Validation:    Accuracy={:.2%}  F1-score={:.2%}'\n",
        "                      .format(dev_acc, dev_f1))\n",
        "\n",
        "        return model\n",
        "\n",
        "    def eval(self, model: 'Sdqc.Model', dataset: 'Sdqc.Dataset') \\\n",
        "            -> (float, float, Dict[str, Dict[str, float]]):\n",
        "        labels, predictions = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data_loader = DataLoader(\n",
        "                dataset, batch_size=self._hparams.batch_size)\n",
        "            for batch in data_loader:\n",
        "                model.eval()\n",
        "                batch_logits = model(batch['emb'], batch['features'])\n",
        "                batch_prediction = torch.argmax(batch_logits, dim=1)\n",
        "\n",
        "                labels.append(batch['label'].data.cpu().numpy())\n",
        "                predictions.append(batch_prediction.data.cpu().numpy())\n",
        "\n",
        "        labels = np.concatenate(labels)\n",
        "        predictions = np.concatenate(predictions)\n",
        "\n",
        "    \n",
        "        # from sklearn.metrics import accuracy_score ,classification_report\n",
        "\n",
        "        # print(classification_report(labels,predictions))\n",
        "\n",
        "        acc = accuracy_score(labels, predictions)\n",
        "        f1 = f1_score(labels, predictions, average='macro')\n",
        "        report = classification_report(\n",
        "            labels, predictions, output_dict=True,\n",
        "            labels=range(len(SdqcInstance.Label)),\n",
        "            target_names=[label.name for label in SdqcInstance.Label])\n",
        "\n",
        "        return acc, f1, report\n",
        "\n",
        "    def predict(self, model: 'Sdqc.Model', post_ids: Iterable[str]) \\\n",
        "            -> Dict[str, Tuple[SdqcInstance.Label,\n",
        "                               Dict[SdqcInstance.Label, float]]]:\n",
        "        instances = [SdqcInstance(post_id) for post_id in post_ids]\n",
        "        dataset = self.Dataset(instances, self._posts, self._post_embeddings,\n",
        "                               self._hparams, self._device)\n",
        "\n",
        "        results = {}\n",
        "        with torch.no_grad():\n",
        "            data_loader = DataLoader(dataset,\n",
        "                                     batch_size=self._hparams.batch_size)\n",
        "            for batch in data_loader:\n",
        "                model.eval()\n",
        "                batch_logits = model(batch['emb'], batch['features'])\n",
        "                batch_probs = F.softmax(batch_logits, dim=1)\n",
        "                batch_prediction = torch.argmax(batch_logits, dim=1)\n",
        "\n",
        "                for post_id, prediction, probs in zip(\n",
        "                        batch['post_id'], batch_prediction, batch_probs):\n",
        "                    results[post_id] = \\\n",
        "                        (SdqcInstance.Label(prediction.item()),\n",
        "                        dict(zip(SdqcInstance.Label, probs.tolist())))\n",
        "        return results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wRI1iOt81TSe",
        "outputId": "1bdfa61c-f42b-4948-9a7d-40075a893a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from datetime import datetime\n",
        "from itertools import chain\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from warnings import filterwarnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "\n",
        "# from src.dataset import check_for_required_external_data_files, load_posts, \\\n",
        "#     load_sdcq_instances, load_verif_instances\n",
        "# from src.sdqc import Sdqc\n",
        "# from src.util import ScalingMode, arrange_folds_for_k_fold_cross_validation, \\\n",
        "#     calculate_post_elmo_embeddings, display_results, filter_instances, \\\n",
        "#     generate_folds_for_k_fold_cross_validation, write_answers_json\n",
        "# from src.verif import Verif\n",
        "\n",
        "time_before = time()\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.fastest = True\n",
        "\n",
        "filterwarnings('ignore', category=UndefinedMetricWarning)\n",
        "\n",
        "NUM_ORGA_REPETITIONS = 5\n",
        "NUM_CV_REPETITIONS = 1\n",
        "NUM_CV_FOLDS = 5\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "\n",
        "check_for_required_external_data_files()\n",
        "\n",
        "posts = load_posts()\n",
        "post_embeddings = calculate_post_elmo_embeddings(\n",
        "    posts,\n",
        "    max_sentence_length=32,\n",
        "    batch_size=512,\n",
        "    scalar_mix_parameters=[0, 0, 0],\n",
        "    device=device)\n",
        "num_emb_dims = next(iter(post_embeddings.values())).shape[0]\n",
        "\n",
        "sdqc_hparams = Sdqc.Hyperparameters(\n",
        "    max_sentence_length=32,\n",
        "    batch_size=512,\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    weight_decay=1e-2,\n",
        "    class_weights=[1, 1, 1, 0.2],\n",
        "    input_num_emb_dims=num_emb_dims,\n",
        "    input_num_aux_dims=11,\n",
        "    input_aux_scaling_features=[4, 5, 6],\n",
        "    input_aux_scaling_mode=ScalingMode.min_max,\n",
        "    conv_num_layers=1,\n",
        "    conv_kernel_sizes=[2, 3],\n",
        "    conv_num_channels=64,\n",
        "    dense_num_layers=3,\n",
        "    dense_num_hidden=128,\n",
        "    dense_dropout=0.5)\n",
        "sdqc = Sdqc(posts, post_embeddings, sdqc_hparams, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading posts...\n",
            "  Number of posts: 8529 (Reddit=1895, Twitter=6634)\n",
            "  Took 4.43s.\n",
            "Calculating post embeddings...\n",
            "  Took 50.43s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRc4xxTS1moz",
        "colab_type": "code",
        "outputId": "433babda-64e3-4a29-bc2f-f53611610d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "\n",
        "sdqc_train_instances, sdqc_dev_instances, sdqc_test_instances = \\\n",
        "    load_sdcq_instances()\n",
        "# print(sdqc_train_instances)\n",
        "# print(\"=======================================================>\")\n",
        "# print(sdqc_dev_instances)\n",
        "# print(\"=======================================================>\")\n",
        "# print(sdqc_test_instances)\n",
        "sdqc_all_instances = list(chain(\n",
        "    sdqc_train_instances, sdqc_dev_instances, sdqc_test_instances))\n",
        "verif_train_instances, verif_dev_instances, verif_test_instances = \\\n",
        "    load_verif_instances()\n",
        "verif_all_instances = list(chain(\n",
        "    verif_train_instances, verif_dev_instances, verif_test_instances))\n",
        "\n",
        "sdqc_times, verif_times = [], []\n",
        "sdqc_dev_accs, sdqc_dev_f1s, sdqc_dev_reports = [], [], []\n",
        "sdqc_test_accs, sdqc_test_f1s, sdqc_test_reports = [], [], []\n",
        "sdqc_cv_accs, sdqc_cv_f1s, sdqc_cv_reports = [], [], []\n",
        "verif_dev_accs, verif_dev_f1s, verif_dev_rmses, verif_dev_reports = \\\n",
        "    [], [], [], []\n",
        "verif_test_accs, verif_test_f1s, verif_test_rmses, verif_test_reports = \\\n",
        "    [], [], [], []\n",
        "verif_cv_accs, verif_cv_f1s, verif_cv_rmses, verif_cv_reports = [], [], [], []\n",
        "\n",
        "answers_dir = Path('answers') / (datetime.utcnow().isoformat() + 'Z')\n",
        "answers_dir.mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "# print()\n",
        "# print('-- Organizer Split ----------------------------------------------------')\n",
        "NUM_ORGA_REPETITIONS = 5\n",
        "for repetition_no in range(NUM_ORGA_REPETITIONS):\n",
        "    print()\n",
        "    # print('## Repetition {}/{}'.format(repetition_no + 1, NUM_ORGA_REPETITIONS))\n",
        "\n",
        "    # print('Task A: SDQC')\n",
        "    t1 = time()\n",
        "    sdqc_train_dataset, sdqc_dev_dataset, sdqc_test_dataset = \\\n",
        "        sdqc.build_datasets(sdqc_train_instances,\n",
        "                            sdqc_dev_instances,\n",
        "                            sdqc_test_instances)\n",
        "    sdqc_model = sdqc.train(sdqc_train_dataset,\n",
        "                            sdqc_dev_dataset,\n",
        "                            print_progress=False)\n",
        "    t2 = time()\n",
        "    sdqc_times.append(t2 - t1)\n",
        "\n",
        "    sdqc_estimates = sdqc.predict(sdqc_model, posts.keys())\n",
        "    if sdqc_dev_dataset:\n",
        "        acc, f1, report = sdqc.eval(sdqc_model, sdqc_dev_dataset)\n",
        "        # print('Validation:  Accuracy={:.1%}  F1-score={:.1%}'.format(acc, f1))\n",
        "        sdqc_dev_accs.append(acc)\n",
        "        sdqc_dev_f1s.append(f1)\n",
        "        sdqc_dev_reports.append(report)\n",
        "    if sdqc_test_dataset:\n",
        "        acc, f1, report = sdqc.eval(sdqc_model, sdqc_test_dataset)\n",
        "        # print('Test:        Accuracy={:.1%}  F1-score={:.1%}'.format(acc, f1))\n",
        "        sdqc_test_accs.append(acc)\n",
        "        sdqc_test_f1s.append(f1)\n",
        "        sdqc_test_reports.append(report)\n",
        "\n",
        "    # model_path = 'data/sdqc_model_{}.pth'.format(repetition_no)\n",
        "    # torch.save(sdqc_model.state_dict(),model_path)\n",
        "\n",
        "print()\n",
        "print('-- Results ------------------------------------------------------------')\n",
        "\n",
        "print()\n",
        "print('# Test')\n",
        "display_results(\n",
        "    sdqc_test_accs, sdqc_test_f1s, sdqc_test_reports,\n",
        "    verif_test_accs, verif_test_f1s, verif_test_rmses, verif_test_reports)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-- Organizer Split ----------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-- Results ------------------------------------------------------------\n",
            "\n",
            "# Test\n",
            "Task A: SDQC\n",
            "  Accuracy: 77.7%±2.2%  F1-Score: 44.8%±3.0%\n",
            "{'accuracy': {'accuracy': '77.7%±2.2%'},\n",
            " 'comment': {'f1-score': '87.6%±1.3%',\n",
            "             'precision': '86.2%±0.8%',\n",
            "             'recall': '89.3%±3.3%',\n",
            "             'support': '147600.0%±0.0%'},\n",
            " 'deny': {'f1-score': '12.7%±3.6%',\n",
            "          'precision': '15.9%±1.9%',\n",
            "          'recall': '12.1%±6.8%',\n",
            "          'support': '10100.0%±0.0%'},\n",
            " 'macro avg': {'f1-score': '44.8%±3.0%',\n",
            "               'precision': '49.9%±4.9%',\n",
            "               'recall': '43.0%±2.5%',\n",
            "               'support': '182700.0%±0.0%'},\n",
            " 'query': {'f1-score': '41.7%±6.8%',\n",
            "           'precision': '56.9%±10.9%',\n",
            "           'recall': '33.5%±7.0%',\n",
            "           'support': '9300.0%±0.0%'},\n",
            " 'support': {'f1-score': '36.9%±2.1%',\n",
            "             'precision': '40.8%±10.2%',\n",
            "             'recall': '37.2%±7.9%',\n",
            "             'support': '15700.0%±0.0%'},\n",
            " 'weighted avg': {'f1-score': '76.8%±1.4%',\n",
            "                  'precision': '76.9%±1.3%',\n",
            "                  'recall': '77.7%±2.2%',\n",
            "                  'support': '182700.0%±0.0%'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15EeqPkZoz6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install allennlp\n",
        "# !pip install git+https://github.com/erikavaris/tokenizer.git \n",
        "# !wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5 -P ./data/external\n",
        "# !wget https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json -P ./data/external"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}